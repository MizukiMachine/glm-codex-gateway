# Z.AI API Key (required)
# Get your API key from https://api.z.ai
ZAI_API_KEY="your-zai-api-key-here"

# Optional: Default model (default: glm-5)
# Available models: glm-5, glm-4.7, glm-4.7-flash, glm-4.7-flashx
GLM_DEFAULT_MODEL="glm-5"

# Optional: Gateway host and port (defaults: 127.0.0.1, 8787)
# GLM_CODEX_GATEWAY_HOST="127.0.0.1"
# GLM_CODEX_GATEWAY_PORT="8787"

# Optional: Z.AI API endpoints
# ZAI_BASE_URL="https://api.z.ai/api/paas/v4"           # Paid tier endpoint
# ZAI_CODING_BASE_URL="https://api.z.ai/api/coding/paas/v4"  # Free tier endpoint (default)

# Optional: Endpoint mode (default: coding-only)
# auto: try both /api/coding/paas/v4 and /api/paas/v4 (requires paid API key)
# base-only: only use /api/paas/v4 (paid tier only)
# coding-only: only use /api/coding/paas/v4 (free tier, default)
# GLM_ZAI_ENDPOINT_MODE="coding-only"

# Optional: Responses fallback mode (default: auto)
# auto: fallback to /chat/completions when /responses is not supported
# always: always use /chat/completions
# never: never fallback, return error directly
# GLM_RESPONSES_FALLBACK_MODE="auto"

# Optional: Stream fallback mode (default: sse)
# sse: simulate SSE streaming for non-streaming responses
# disabled: disable SSE simulation, use non-streaming response
# GLM_RESPONSES_STREAM_FALLBACK_MODE="sse"

# Optional: Log level (default: info)
# Available: info, debug, silent
# GLM_CODEX_GATEWAY_LOG_LEVEL="info"

# Optional: Request timeout in milliseconds (default: 180000)
# GLM_CODEX_GATEWAY_TIMEOUT_MS="180000"
